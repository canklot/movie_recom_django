# -*- coding: utf-8 -*-
"""mc_basic_retrieval_tr.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13Zq6e_mDe0kkyQ3V4BdYgi9PCTCFuXyh

# Filim öneri sistemi
"""

""" 
!pip install  tensorflow-recommenders
!pip install  tensorflow-datasets 
"""

import os
import pprint
import tempfile
from typing import Dict, Text
import numpy as np
import tensorflow as tf
import tensorflow_datasets as tfds
import tensorflow_recommenders as tfrs


def runall(yeni_girdi_list):
    
  """## Verisetini hazırlama

  https://files.grouplens.org/datasets/movielens/ml-100k-README.txt

  Tfds modülü kullanmaya hazır veri setleri barındıran bir modül.
  Bu modülün içinden 100 bin film değerlendirmesine sahip olan movielens verisetini kullanıcaz.
  Bazı veri setleri eğitim ve test olarak iki parça halinde gelirken bu veriseti tüm veriyi eğitim setinin içine koymuş.
  Veri seti baya eski bir veri seti ve sadece 99 yılına kadar olan filimler var.
  """

  gecmis = tfds.load("movielens/100k-ratings", split="train")
  filimler = tfds.load("movielens/100k-movies", split="train")

  """Veri setimiz şuan daha iyi performans almak için bir array içinde değil PrefetchDataset sınıfı içinde.
  Bu yüzden indeks ile erişip ekrana basamıyoruz.
  Ama bir iterator ile erişip ekrana basınca verisetimizde film, puan, kullanıcı yaşı gibi veriler olduğunu görüyoruz.
  PrefetchDataset sınıfı büyük verisetleri ile uğraşırken, modelimiz verisetinin ilk parçasını öğrenmeye çalışırken 
  arka planda ikinci parçayı asenkron olarak getiren ve daha verimli öğrenmeyi sağlayan bir sınıftır
  """

  """ for x in gecmis.take(1).as_numpy_iterator():
    pprint.pprint("Ratings veri setinin örnek bir elemenı:")
    pprint.pprint(x) """

  """ for x in filimler.take(1).as_numpy_iterator():
    pprint.pprint("Movies veri setinin örnek bir elemenı:")
    pprint.pprint(x) """

  """Netflix de oylama sistemi bulunmadığından bu projede sadece izleme geçmişini kullanarak öneride bulunacağız.
  Bu yüzden veri setimizde sadece film ismi ve kullanıcı id kalacak şekilde atama yapıyoruz.
  tf.map fonksiyonu verisetindeki her elemanı tek tek parametre olarak verilen fonksiyona uygulayan bir fonksiyondur.
  """

  gecmis = gecmis.map(lambda x: {
      "movie_title": x["movie_title"],
      "user_id": x["user_id"],
  })
  filimler = filimler.map(lambda x: x["movie_title"])

  """ for x in gecmis.take(1).as_numpy_iterator():
    pprint.pprint("Ratings veri setinin gereksiz kısımları sildikten sonraki bir elemenı:")
    pprint.pprint(x) """

  """### Veri setine kendi kullanıcımızı ekleme"""

  # Modeli test edebilmek için veri setine sadece bilim kurgu veya romantik film izlemiş bir kullanıcı ekliyoruz
  # Bilim kurgu filmleri

  """   yeni_girdi1= {
      "movie_title": "Twelve Monkeys (1995)",
      "user_id": "12345678", 
  }
  yeni_girdi2 = {
      "movie_title": "Terminator 2: Judgment Day (1991)",
      "user_id": "12345678",
  }
  yeni_girdi3 = {
      "movie_title": "Alien 3 (1992)",
      "user_id": "12345678",
  }
  yeni_girdi4 = {
      "movie_title": "Jurassic Park (1993)",
      "user_id": "12345678",
  }
  yeni_girdi5 = {
      "movie_title": "Men in Black (1997)",
      "user_id": "12345678", 
  }  """

  # Romantik filimler
  """ yeni_girdi1= {
      "movie_title": "Meet Joe Black (1998)",
      "user_id": "12345678", 
  }

  yeni_girdi2 = {
      "movie_title": "10 Things I Hate About You (1999)",
      "user_id": "12345678",
  }
  yeni_girdi3 = {
      "movie_title": "Four Weddings and a Funeral (1994)",
      "user_id": "12345678", 
  }
  yeni_girdi4 = {
      "movie_title": "Jerry Maguire (1996)",
      "user_id": "12345678",
  }
  yeni_girdi5 = {
      "movie_title": "Pretty Woman (1990)",
      "user_id": "12345678",
  }
  """
  #yeni_girdi_list = [yeni_girdi1,yeni_girdi2,yeni_girdi3,yeni_girdi4,yeni_girdi5]
  yeni_girdi_ds_list = []

  # Yeni eklediğimiz verileri tf.Dataset objesine dönüştürüyoruz
  for yeni_girdi in yeni_girdi_list:
    yeni_girdi_ds_list.append(tf.data.Dataset.from_tensors(yeni_girdi))

  # Dönüştürülmüş verisetini eski veri setine ekliyoruz
  for yeni_girdi_ds in yeni_girdi_ds_list:
    gecmis = gecmis.concatenate(yeni_girdi_ds)

  # Veri setini %80 eğitim %20 test olacak şekilde ayarlıyoruz ve karıştırıyoruz
  tf.random.set_seed(33)
  shuffled = gecmis.shuffle(100_000, seed=30, reshuffle_each_iteration=False)

  train = shuffled.take(80_000)
  test = shuffled.skip(80_000).take(20_000)

  # Tüm kullanıcı id'lerinin ve filmlerin listesini çıkarıyoruz. 
  # İleride bu listeyi kullanarak her bir filme ve kullanıcıya bir sayı atıyacağız.
  # Bu atama sayesinden model uzun stringler ile uğraşmak yerine onları temsil eden sayılar ile uğraşıcak.
  # Filmler listesinden her filmden bir tane olması lazım ama 10 kadar film birden fazla var. Bu yüzden np.uniqe ile duplicate filmleri siliyoruz
  filim_isimleri = filimler.batch(1_000)
  kullanıcı_idleri = gecmis.batch(1_000_000).map(lambda x: x["user_id"])

  filim_isimleri_tekil = np.unique(np.concatenate(list(filim_isimleri)))
  kullanıcı_idleri_tekil = np.unique(np.concatenate(list(kullanıcı_idleri)))

  """   with open("filim_isimleri_tekil.txt", "w",encoding='utf-8') as file1:
    for element in filim_isimleri_tekil:
        
        file1.write(element) """
 

  # Listemizi ekrana basıp bir kontrol edelim
  """ print(filim_isimleri_tekil[:10])
  kullanıcı_idleri_tekil[:10] """

  """## Model tasarımı"""

  # Modelimizin ilk katmanı girdilerimiz yani filmler olucak.
  # Model uzun string film isimleri ile uğraşmasın ve daha verimli çalışması için
  # StringLookUp fonksiyonu ile her filme bir sayı atıyoruz.
  # İkinci katmanda ise sayıları 2 boyutlu bir vektöre dönüştürüyoruz.
  # Bu sayede benzer filimler 2 boyutlu düzlemde bir birine yakın yerleştirilebilir.
  # Ara katman olarak ise 32 node ekliyoruz. Daha fazla node daha karmaşık problemleri öğrenebilir ama
  # Overfittinge daha yatkın ve öğrenme daha yavaş olacaktır
  user_model = tf.keras.Sequential([
      tf.keras.layers.StringLookup(vocabulary=kullanıcı_idleri_tekil, mask_token=None),
      # Bilinmeyen tokenler için +1 node ekliyoruz
      tf.keras.layers.Embedding(len(kullanıcı_idleri_tekil) + 1,32)
  ])

  # Kullanıcılar için oluşturduğumuz modelin aynısını filmler için de oluşturuyoruz.
  movie_model = tf.keras.Sequential([
    tf.keras.layers.StringLookup(
        vocabulary=filim_isimleri_tekil, mask_token=None),
    tf.keras.layers.Embedding(len(filim_isimleri_tekil) + 1,32)
  ])

  # Modelimizin performansını ölçmek için tüm filmleri modelimize sokuyoruz
  # Kullanıcının izlediği filmlerin benzerlik oranı izlemediği filmlerden yüksek ise modelimiz başarılı demektir.
  # FactorizedTopK fonksiyonu kullanıcının izlediği filmlerin ne sıklıkla ilk k sıraya girdiğini hesaplar.
  metrics = tfrs.metrics.FactorizedTopK(
    candidates=filimler.batch(128).map(movie_model)
  )

  # Modeli eğitirken kullanılan loss fonksiyonu olarak ise tfrs.tasks.Retrieval katmanı kullanıyoruz
  # Bu katman loss ve metrics değerlerini ortak hesaplayan ve verilen sorgudaki filmlerin yakınlık değerini maximize etmeye ve
  # Diğer adaylar ile benzerlik oranını minimize etmeye çalışan bir katmandır.
  task = tfrs.tasks.Retrieval(
    metrics=metrics
  )

  # Modelimizi tfrs.models kalıtım alan bir sınıf içinde birleştiriyoruz
  class FilmOneriModeli(tfrs.Model):

    def __init__(self, user_model, movie_model):
      super().__init__()
      self.movie_model: tf.keras.Model = movie_model
      self.user_model: tf.keras.Model = user_model
      self.task: tf.keras.layers.Layer = task

    def compute_loss(self, features: Dict[Text, tf.Tensor], training=False) -> tf.Tensor:
      user_embeddings = self.user_model(features["user_id"])
      positive_movie_embeddings = self.movie_model(features["movie_title"])
      return self.task(user_embeddings, positive_movie_embeddings)

  

  """## Eğitim"""

  # Model sınıfımızdan bir obje oluşturuyoruz ve compile ediyoruz
  model = FilmOneriModeli(user_model, movie_model)
  
  model.compile(optimizer=tf.keras.optimizers.Adagrad(learning_rate=0.1))
  
  # Son olarak veri setimizi karıştırıp gruplayıp önbelleğe alıyoruz.
  cached_train = train.shuffle(100_000).batch(8192).cache()
  cached_test = test.batch(4096).cache()

  # Modeli eğitiyoruz
  model.fit(cached_train, epochs=3) # test ederken beklememek icin 1 yaptım beni 3 falkan yap

  # Modelin daha önce hiç görmediği veriler ile test ediyoruz
  # İlk 50 öneri içinde başarı oranı yüzde 10
  model.evaluate(cached_test, return_dict=True)
 
  """## Tahmin yaptırma

  Eğitilen modelden öneri istemek için `tfrs.layers.factorized_top_k.BruteForce` layerını kullanıyoruz.
  """

  index = tfrs.layers.factorized_top_k.BruteForce(model.user_model)
  index.index_from_dataset(
    tf.data.Dataset.zip((filimler.batch(100), filimler.batch(100).map(model.movie_model)))
  )

  # Kullanıcı id'si ile modele tahmin yaptırtıyoruz
  _, oneriler = index(tf.constant(["12345678"]))
  print(f"Recommendations for user:")
  for x in range(10):
    print(oneriler[0,x])

  return oneriler


  
  """ ## Kaydetme

  # Modelimizi daha sonra tekrar kullanabilmek için kaydediyoruz
  
  with open("mymodel", "w") as file1:
    path = os.path.join(os.getcwdb(), "file1")
    tf.saved_model.save(index, path)

    # Modeli dosyadan okumak için saved_model.load kullanıyoruz
    loaded = tf.saved_model.load(path)

    # Yüklediğimiz modele kullanıcı id'si verip tahmin yapmasını istiyoruz.
    scores, oneriler = loaded(["12345678"])

    print(f"Recommendations for user:")
    for x in range(9):
      print(oneriler[0,x])
 """
if __name__ == "__main__":
  yeni_girdi_list = [
        {"movie_title": "Twelve Monkeys (1995)",
         "user_id": "12345678"},
        {"movie_title": "Terminator 2: Judgment Day (1991)",
         "user_id": "12345678"},
        {"movie_title": "Alien 3 (1992)",
         "user_id": "12345678"},
        {"movie_title": "Jurassic Park (1993)",
         "user_id": "12345678"},
        {"movie_title": "Men in Black (1997)",
         "user_id": "12345678"} 
    ]
  runall(yeni_girdi_list)